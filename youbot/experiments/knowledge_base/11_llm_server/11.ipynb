{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n",
    "\n",
    "use out of the box NER to extract entity names, turn to LLM to label with custom set of entities\n",
    "\n",
    "use outline to run against LM Studio (presumably quantized 7b models)\n",
    "\n",
    "\n",
    "### Observations\n",
    "\n",
    "Memory requirements, at least via spacy-llm, seem really high for running local models\n",
    "\n",
    "unclear if that's because of inefficiencies stemming from spacy library, or inherent to running models\n",
    "\n",
    "wrapping models in libraries seems to generate inefficient use of api's, getting lots of rate limit errors etc with spacy and outline\n",
    "\n",
    "lm studio seems like a good approach to doing openai-like calls without incurring cost or rate limits\n",
    "\n",
    "### renting gpus\n",
    "\n",
    "paperspace is probably still best vs other options, best ui, relatively easy to get notebooks running. some thrash in disconnected kernels seeming to continue to run workloads\n",
    "\n",
    "azure is very enterprisey still, not friendly to solo dev\n",
    "\n",
    "google collab is underbaked, keeps you in their sub-par notebook environment. feels like abandonware/promotionware\n",
    "\n",
    "probably the pricing model everyone lands on is subscription. access to higher GPU RAM machines is gated on higher subscription costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "CONFIG_CONTENT = \"\"\"\n",
    "\n",
    "[nlp]\n",
    "lang = \"en\"\n",
    "pipeline = [\"ner\"]\n",
    "\n",
    "[components]\n",
    "\n",
    "[components.ner]\n",
    "source = \"en_core_web_md\"\n",
    "\n",
    "\n",
    "[initialize]\n",
    "vectors = \"en_core_web_md\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('config.cfg', 'w') as f:\n",
    "    f.write(CONFIG_CONTENT)\n",
    "    \n",
    "    \n",
    "DATA_SOURCE_DIR = ''\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import spacy_llm\n",
    "from spacy_llm.util import assemble\n",
    "import spacy\n",
    "\n",
    "\n",
    "\n",
    "config = \"config.cfg\"\n",
    "\n",
    "model_name = \"en_core_web_md\"\n",
    "try:\n",
    "    nlp = assemble(config)\n",
    "except OSError:\n",
    "    spacy.cli.download(model_name)\n",
    "    nlp = assemble(config)\n",
    "\n",
    "# set log level to stream to STDOUT\n",
    "spacy_llm.logger.addHandler(logging.StreamHandler())\n",
    "spacy_llm.logger.setLevel(logging.DEBUG)\n",
    "\n",
    "nlp = assemble(\"config.cfg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine NER\n",
    "\n",
    "resolve disagreements by iterating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:00<00:00, 201.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_PATH = 'msgs.pkl'\n",
    "\n",
    "with open(DATA_PATH, 'rb') as f:\n",
    "    docs_df = pd.read_pickle(f)\n",
    "    \n",
    "docs = docs_df[0].tolist()\n",
    "\n",
    "\n",
    "entities_rows = []\n",
    "for doc in tqdm(docs):\n",
    "    enriched_doc = nlp(doc)\n",
    "    ents = enriched_doc.ents\n",
    "    for ent in ents:\n",
    "        entities_rows.append({\"name\": ent.text, \"label\": ent.label_, \"fact\": doc})  # type: ignore\n",
    "\n",
    "\n",
    "entities_df = pd.DataFrame(entities_rows)\n",
    "\n",
    "with open('entities.pkl', 'wb') as f:\n",
    "    entities_df.to_pickle(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolve inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsampling facts\n",
      "considering name 'Test Event'. round 1, label choices: ['PERSON', 'PET', 'ORG', 'PRODUCT', 'WEBSITE', 'GPE', 'TVSHOW', 'BOOK', 'MOVIE', 'TECHNICALCONCEPT', 'EVENT']. Number of facts: 1\n",
      "***\n",
      "name: 'Test Event'\n",
      "\n",
      "facts: Google email 'tombedor@gmail.com' was linked for calendar event usage. Successfully created events 'Test Event' and 'Leave for LA' on request.\n",
      "\n",
      "response: EVENT\n",
      "\n",
      "I classified \"Test Event\" as an EVENT because it is mentioned in the context of being created on a calendar, which suggests that it is a scheduled occurrence or meeting, rather than any other type of entity. The fact that it was created along with another event (\"Leave for LA\") further supports this classification.\n",
      "\n",
      "***\n",
      "\n",
      "***\n",
      "name: 'Test Event'\n",
      "\n",
      "facts: Google email 'tombedor@gmail.com' was linked for calendar event usage. Successfully created events 'Test Event' and 'Leave for LA' on request.\n",
      "\n",
      "response: EVENT\n",
      "\n",
      "I classified \"Test Event\" as an EVENT because it is mentioned in the context of calendar event usage and creation. The phrase \"Successfully created events 'Test Event' and 'Leave for LA'\" specifically references the creation of events, which suggests that \"Test Event\" is a type of event rather than any other entity type. Additionally, the use of the word \"event\" in the sentence implies that \"Test Event\" is an instance of an event, further supporting this classification.\n",
      "\n",
      "***\n",
      "\n",
      "***\n",
      "name: 'Test Event'\n",
      "\n",
      "facts: Google email 'tombedor@gmail.com' was linked for calendar event usage. Successfully created events 'Test Event' and 'Leave for LA' on request.\n",
      "\n",
      "response: EVENT\n",
      "\n",
      "I classified 'Test Event' as an EVENT because it is mentioned in the context of being created on Google calendar. The sentence \"Successfully created events 'Test Event' and 'Leave for LA' on request\" suggests that 'Test Event' is a specific occurrence or appointment, which is typical of events. Additionally, the word \"event\" has a clear meaning in this context, making it a more likely candidate than other options.\n",
      "\n",
      "***\n",
      "\n",
      "winning candidate found: EVENT\n",
      "upsampling facts\n",
      "considering name .ghtoken. round 1, label choices: ['MOVIE', 'WEBSITE', 'EVENT', 'PERSON', 'TECHNICALCONCEPT', 'PRODUCT', 'ORG', 'BOOK', 'TVSHOW', 'GPE', 'PET']. Number of facts: 1\n",
      "***\n",
      "name: .ghtoken\n",
      "\n",
      "facts: Tom asked about configuring the 'gh' command-line tool (GitHub CLI) to change the user context based on the current directory. Specifically, he wanted to incorporate this into the `cd` command. In response, I proposed a Bash/Zsh function that overwrites `cd` to modify the `GH_TOKEN` environment variable based on the contents of a '.ghtoken' file in the new directory. This is an advanced and custom usage of GitHub CLI combined with shell scripting that explores dynamic adaptation to the directory context.\n",
      "\n",
      "response: TECHNICALCONCEPT\n",
      "\n",
      "I classified \".ghtoken\" as a TECHNICALCONCEPT because it is a file used to store a GitHub token, which is a technical concept related to software development and version control. The context in which it was mentioned (configuring the 'gh' command-line tool and incorporating it into the `cd` command) further supports this classification, as it involves advanced shell scripting and customization of GitHub CLI usage.\n",
      "\n",
      "***\n",
      "\n",
      "***\n",
      "name: .ghtoken\n",
      "\n",
      "facts: Tom asked about configuring the 'gh' command-line tool (GitHub CLI) to change the user context based on the current directory. Specifically, he wanted to incorporate this into the `cd` command. In response, I proposed a Bash/Zsh function that overwrites `cd` to modify the `GH_TOKEN` environment variable based on the contents of a '.ghtoken' file in the new directory. This is an advanced and custom usage of GitHub CLI combined with shell scripting that explores dynamic adaptation to the directory context.\n",
      "\n",
      "response: TECHNICALCONCEPT\n",
      "\n",
      "My classification is based on the context in which the term '.ghtoken' was mentioned. The conversation revolves around configuring a command-line tool (GitHub CLI) and writing a Bash/ Zsh function to modify environment variables based on the current directory. This suggests that '.ghtoken' is not a physical entity, but rather a concept or variable used within the GitHub CLI and shell scripting context. Therefore, I classify it as a TECHNICALCONCEPT.\n",
      "\n",
      "***\n",
      "\n",
      "***\n",
      "name: .ghtoken\n",
      "\n",
      "facts: Tom asked about configuring the 'gh' command-line tool (GitHub CLI) to change the user context based on the current directory. Specifically, he wanted to incorporate this into the `cd` command. In response, I proposed a Bash/Zsh function that overwrites `cd` to modify the `GH_TOKEN` environment variable based on the contents of a '.ghtoken' file in the new directory. This is an advanced and custom usage of GitHub CLI combined with shell scripting that explores dynamic adaptation to the directory context.\n",
      "\n",
      "response: TECHNICALCONCEPT\n",
      "\n",
      "I classify '.ghtoken' as a TECHNICALCONCEPT because it appears to be a configuration file or token used in conjunction with the GitHub CLI ('gh' command-line tool) and shell scripting (Bash/Zsh). The discussion revolves around using this token to change the user context based on the current directory, which is an advanced and custom usage of GitHub CLI combined with shell scripting. This classification is supported by the chat logs, where Tom asked about configuring the 'gh' tool and the proposed solution involves modifying the `GH_TOKEN` environment variable based on the contents of a '.ghtoken' file in the new directory.\n",
      "\n",
      "***\n",
      "\n",
      "winning candidate found: TECHNICALCONCEPT\n",
      "upsampling facts\n",
      "considering name 1Password. round 1, label choices: ['TECHNICALCONCEPT', 'MOVIE', 'WEBSITE', 'TVSHOW', 'PET', 'GPE', 'PERSON', 'EVENT', 'BOOK', 'ORG', 'PRODUCT']. Number of facts: 1\n",
      "***\n",
      "name: 1Password\n",
      "\n",
      "facts: Tom asked about the difference between using `eval $(op signin)` and `op signin`. The `op` command is part of 1Password's command-line tool, and `signin` is used to authenticate. Simply using `op signin` outputs an export statement to set an environment variable for authentication, but it doesn't execute it. In contrast, `eval $(op signin)` evaluates its output as a shell command and executes it. Therefore, this command both runs `op signin` and executes its output, effectively authenticating the session within the shell environment.\n",
      "\n",
      "response: PRODUCT\n",
      "\n",
      "I classify 1Password as a PRODUCT because it is a software application designed to manage and secure passwords, credit cards, and other sensitive information. The provided chat logs describe the `op` command-line tool, which is part of the 1Password product, used for authentication and session management. The logs also mention specific features and functionality of the product, such as exporting environment variables for authentication. This indicates that 1Password is a software product designed to provide a service or utility to users, rather than a movie, person, event, or other non-product entity.\n",
      "\n",
      "***\n",
      "\n",
      "***\n",
      "name: 1Password\n",
      "\n",
      "facts: Tom asked about the difference between using `eval $(op signin)` and `op signin`. The `op` command is part of 1Password's command-line tool, and `signin` is used to authenticate. Simply using `op signin` outputs an export statement to set an environment variable for authentication, but it doesn't execute it. In contrast, `eval $(op signin)` evaluates its output as a shell command and executes it. Therefore, this command both runs `op signin` and executes its output, effectively authenticating the session within the shell environment.\n",
      "\n",
      "response: PRODUCT\n",
      "\n",
      "I classify 1Password as a PRODUCT because it is a software application designed to manage and secure passwords and other sensitive information. The context provided in the chat log also suggests that 1Password has a command-line tool, which is typical for products. Additionally, the functionality described in the log, such as authenticating sessions and setting environment variables, is characteristic of product software rather than an event, person, or concept.\n",
      "\n",
      "***\n",
      "\n",
      "***\n",
      "name: 1Password\n",
      "\n",
      "facts: Tom asked about the difference between using `eval $(op signin)` and `op signin`. The `op` command is part of 1Password's command-line tool, and `signin` is used to authenticate. Simply using `op signin` outputs an export statement to set an environment variable for authentication, but it doesn't execute it. In contrast, `eval $(op signin)` evaluates its output as a shell command and executes it. Therefore, this command both runs `op signin` and executes its output, effectively authenticating the session within the shell environment.\n",
      "\n",
      "response: PRODUCT\n",
      "\n",
      "I classify 1Password as a product because it is a software application designed to securely store and manage passwords, credit cards, and other sensitive information. The fact that it has a command-line tool and can be used for authentication purposes further supports its classification as a product. Additionally, the context of Tom's question suggests that 1Password is being used in a technical sense, which is consistent with the concept of a software product.\n",
      "\n",
      "***\n",
      "\n",
      "winning candidate found: PRODUCT\n",
      "upsampling facts\n",
      "considering name AI. round 1, label choices: ['MOVIE', 'PET', 'WEBSITE', 'TVSHOW', 'PERSON', 'PRODUCT', 'GPE', 'BOOK', 'TECHNICALCONCEPT', 'EVENT', 'ORG']. Number of facts: 1\n",
      "***\n",
      "name: AI\n",
      "\n",
      "facts: On 2024-01-26, Tom suggested an enhancement to our task management system by proposing the addition of a 'notes' feature. This led to the creation of Task ID: 5, described as 'Add a notes column to the tasks table, so the AI can take a note about the tasks'. This task aims to enrich our task narratives with added context through notes. Following this, we returned to Task ID: 4 which tests our system's main operation: managing the life-cycle of a task. This gives our system a full cycle exercise, from creating a task, marking it as 'in-progress', to marking it as 'completed'.\n",
      "\n",
      "response: TECHNICALCONCEPT\n",
      "\n",
      "I classify AI as a TECHNICALCONCEPT because in the given context, it refers to an artificial intelligence that can take notes about tasks and be integrated into a task management system. The mention of \"AI\" specifically implies a technical concept, rather than a physical entity or a work of fiction (GPE, PERSON, MOVIE, BOOK, TVSHOW). The fact that it's being discussed in the context of software development and feature enhancement further supports this classification.\n",
      "\n",
      "***\n",
      "\n",
      "***\n",
      "name: AI\n",
      "\n",
      "facts: On 2024-01-26, Tom suggested an enhancement to our task management system by proposing the addition of a 'notes' feature. This led to the creation of Task ID: 5, described as 'Add a notes column to the tasks table, so the AI can take a note about the tasks'. This task aims to enrich our task narratives with added context through notes. Following this, we returned to Task ID: 4 which tests our system's main operation: managing the life-cycle of a task. This gives our system a full cycle exercise, from creating a task, marking it as 'in-progress', to marking it as 'completed'.\n",
      "\n",
      "response: TECHNICALCONCEPT\n",
      "\n",
      "I classified AI as a TECHNICALCONCEPT because it is a term used in the context of technology and computer science to describe artificial intelligence. The mention of adding a 'notes' feature to a task management system, which involves an AI taking notes about tasks, further supports this classification. Additionally, the description of the task aims to enrich task narratives with added context through notes, indicating that AI is being used as a tool for processing and analyzing data. Overall, the context in which AI is mentioned suggests that it is a technical concept rather than another type of entity such as a person or organization.\n",
      "\n",
      "***\n",
      "\n",
      "***\n",
      "name: AI\n",
      "\n",
      "facts: On 2024-01-26, Tom suggested an enhancement to our task management system by proposing the addition of a 'notes' feature. This led to the creation of Task ID: 5, described as 'Add a notes column to the tasks table, so the AI can take a note about the tasks'. This task aims to enrich our task narratives with added context through notes. Following this, we returned to Task ID: 4 which tests our system's main operation: managing the life-cycle of a task. This gives our system a full cycle exercise, from creating a task, marking it as 'in-progress', to marking it as 'completed'.\n",
      "\n",
      "response: TECHNICALCONCEPT\n",
      "\n",
      "My reasoning is based on the context provided. The mention of \"AI\" in the task description, specifically in the phrase \"'so the AI can take a note about the tasks'\", suggests that AI stands for Artificial Intelligence. In this context, AI refers to a technical concept rather than an entity from other categories (e.g., a person, organization, or location). The addition of a \"notes\" feature is intended to enrich task narratives with added context through notes, which is a functionality typical of technical concepts like AI-powered systems.\n",
      "\n",
      "***\n",
      "\n",
      "winning candidate found: TECHNICALCONCEPT\n",
      "upsampling facts\n",
      "considering name API. round 1, label choices: ['GPE', 'MOVIE', 'BOOK', 'TVSHOW', 'PET', 'ORG', 'PRODUCT', 'TECHNICALCONCEPT', 'EVENT', 'PERSON', 'WEBSITE']. Number of facts: 2\n",
      "***\n",
      "name: API\n",
      "\n",
      "facts: Tom has shown interest in managing databases for his digital projects. He inquired about integrating a database into his DigitalOcean App specification. He also asked about creating an auto-incrementing integer ID in SQLAlchemy. Tom further explored different services for API setup, including AWS Lambda and API Gateway, and whether DigitalOcean offers a similar service. His inquiries largely revolve around refining his API setup and managing databases effectively.\n",
      "\n",
      "response: TECHNICALCONCEPT\n",
      "\n",
      "I classified the entity \"API\" as a TECHNICALCONCEPT because Tom's inquiries and interest in managing databases for his digital projects suggest that he is focused on using APIs as a technical tool to integrate database functionality into his applications. His exploration of AWS Lambda, API Gateway, and DigitalOcean services further supports this classification, as these are all technical concepts related to API setup and management. In this context, the term \"API\" refers to an Application Programming Interface, which is a set of defined rules that enable different applications to communicate with each other.\n",
      "\n",
      "***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# refine entity labels\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "os.environ['OPENAI_BASE_URL'] = \"http://localhost:1234/v1\"\n",
    "os.environ['OPENAI_API_KEY'] = \"lm-studio\"\n",
    "client = OpenAI()\n",
    "model = client.models.list().data[0].id\n",
    "\n",
    "VALID_LABELS = [\n",
    "    \"PERSON\",\n",
    "    \"PET\",\n",
    "    \"ORG\",\n",
    "    \"PRODUCT\",\n",
    "    \"WEBSITE\",\n",
    "    \"GPE\",\n",
    "    \"TVSHOW\",\n",
    "    \"BOOK\",\n",
    "    \"MOVIE\",\n",
    "    \"TECHNICALCONCEPT\",\n",
    "    \"EVENT\"\n",
    "]\n",
    "\n",
    "DISCARD_LABELS = [\n",
    "    \"CARDINAL\",\n",
    "    \"DATE\",\n",
    "    \"TIME\"\n",
    "]\n",
    "\n",
    "# discard entities with any of the discard labels\n",
    "entities_df = entities_df[~entities_df[\"label\"].isin(DISCARD_LABELS)]\n",
    "\n",
    "# get list of unique labels:\n",
    "unique_labels = entities_df[\"name\"].unique()\n",
    "\n",
    "# new dataframe, with a name column and a facts column, which contains the array of facts\n",
    "name_df = entities_df.groupby(\"name\")[\"fact\"].apply(lambda x: pd.unique(x)).reset_index()\n",
    "\n",
    "\n",
    "def chunks(l, n): \n",
    "    # looping till length l \n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n] \n",
    "# for all entities labeled as person, disambiguate between pets and people \n",
    "def refine_labels(row: pd.Series, current_round = 1, label_choices = VALID_LABELS):    \n",
    "    MAX_ROUNDS = 3\n",
    "    MAX_BATCH_FACTS = 1\n",
    "    MIN_FACTS = 3\n",
    "    \n",
    "    name = row[\"name\"]\n",
    "    fact_set = row[\"fact\"]\n",
    "    \n",
    "    if len(fact_set) < MIN_FACTS:\n",
    "        print('upsampling facts')\n",
    "        fact_list = list(fact_set) * ceil(MIN_FACTS / len(fact_set))\n",
    "    else:\n",
    "        fact_list = list(fact_set)\n",
    "    print(f\"considering name {name}. round {current_round}, label choices: {label_choices}. Number of facts: {len(fact_set)}\")\n",
    "    \n",
    "    \n",
    "    if current_round > MAX_ROUNDS:\n",
    "        print(\"exhausted retries for name {name}, remaining choices are: {label_choices}\")\n",
    "        return label_choices\n",
    "    \n",
    "    \n",
    "    random.shuffle(fact_set)\n",
    "    random.shuffle(label_choices)\n",
    "    labels_str = \", \".join(label_choices)\n",
    "    \n",
    "    candidates = set()\n",
    "    for fact_chunk in chunks(fact_list, MAX_BATCH_FACTS):\n",
    "        facts_str = \"\\n\".join(set(fact_chunk))\n",
    "        prompt = f\"\"\"You are an entity resolution assistant. \n",
    "        You must classify the entity with name = {name}\n",
    "        \n",
    "        The valid choices are: {labels_str}\n",
    "        \n",
    "        \n",
    "        Use both your inherent knowledge, and these facts derived from chat logs:\n",
    "        {facts_str}\n",
    "        \n",
    "        Your response should begin with just one word from the following choices: {labels_str}, then should follow with an explanation of your reasoning.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(model=model, messages=[{\"role\": \"user\", \"content\": prompt}]).choices[0].message.content\n",
    "        assert(response)\n",
    "        print(f\"***\\nname: {name}\\n\\nfacts: {facts_str}\\n\\nresponse: {response}\\n\\n***\\n\")\n",
    "        \n",
    "        # whichever label appears first wins\n",
    "        winner = None\n",
    "        min_idx = float('inf')\n",
    "        for label in label_choices:\n",
    "            if label in response:\n",
    "                idx = response.index(label)\n",
    "                if idx < min_idx:\n",
    "                    min_idx = idx\n",
    "                    winner = label\n",
    "        if winner:\n",
    "            candidates.add(winner)\n",
    "    if len(candidates) == 0:\n",
    "        print('no candidates found, retrying')\n",
    "        return refine_labels(row, current_round + 1, label_choices)\n",
    "    elif len(candidates) == 1:\n",
    "        winner = candidates.pop()\n",
    "        print(f\"winning candidate found: {winner}\")\n",
    "        return winner\n",
    "    else:\n",
    "        print(f\"narrowed to candidates: {candidates}\")\n",
    "        \n",
    "        return refine_labels(row, current_round + 1, list(candidates))\n",
    "        \n",
    "# curl http://localhost:1234/v1/chat/completions \\\n",
    "#   -H \"Content-Type: application/json\" \\\n",
    "#   -d '{ \n",
    "#     \"model\": \"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n",
    "#     \"messages\": [ \n",
    "#       { \"role\": \"system\", \"content\": \"Always answer in rhymes.\" },\n",
    "#       { \"role\": \"user\", \"content\": \"Introduce yourself.\" }\n",
    "#     ], \n",
    "#     \"temperature\": 0.7, \n",
    "#     \"max_tokens\": -1,\n",
    "#     \"stream\": true\n",
    "# }'\n",
    "\n",
    "# apply the refine_labels function to each row in the name_df\n",
    "name_df['label'] = name_df.apply(refine_labels, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# refine entity labels\n",
    "from email import generator\n",
    "import os\n",
    "from openai import AsyncOpenAI\n",
    "import pandas as pd\n",
    "\n",
    "person_labels = [\"PERSON\", \"PET\"]\n",
    "\n",
    "os.environ['OPENAI_BASE_URL'] = \"http://localhost:1234/v1\"\n",
    "os.environ['OPENAI_API_KEY'] = \"lm-studio\"\n",
    "# model = \"LM Studio Community/Meta-Llama-3-8B-Instruct-GGUF\"\n",
    "model = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "\n",
    "# for all entities labeled as person, disambiguate between pets and people \n",
    "def refine_labels(row: pd.Series):\n",
    "    name = row[\"name\"]\n",
    "    label = row[\"label\"]\n",
    "    if label == \"PERSON\" and name != \"Tom\":\n",
    "        prompt = f\"\"\"You are an entity resolution assistant. \n",
    "        You must classify the entity with name = {name}\n",
    "        \n",
    "        The valid choices are: PERSON, PET, UNKNOWN\n",
    "        A previous classifier labeled this entity as: {label}\n",
    "        \n",
    "        Use both your inherent knowledge, and these facts derived from chat logs:\n",
    "        {row[\"fact\"]} \n",
    "        \n",
    "        The first word of your response should be one of the following: PERSON, PET, UNKNOWN\n",
    "        Follow this with explanation of your reasoning.\n",
    "        \"\"\"\n",
    "\n",
    "        answer = client.chat.completions.create(model=model, messages=[{\"role\": \"user\", \"content\": prompt}]).choices[0].message.content\n",
    "        print(f\"NAME: {name}\\nFact:{row['fact']}.\\n\\n***\\nWINNER = {answer}\\n***\")\n",
    "\n",
    "        return answer\n",
    "    else:\n",
    "        return label\n",
    "\n",
    "        \n",
    "# apply the function to all rows\n",
    "# add column, new_label\n",
    "entities_df[\"new_label\"] = entities_df.apply(refine_labels, axis=1)\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
