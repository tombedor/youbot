{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n",
    "\n",
    "use out of the box NER to extract entity names, turn to LLM to label with custom set of entities\n",
    "\n",
    "use outline to run against LM Studio (presumably quantized 7b models)\n",
    "\n",
    "\n",
    "### Observations\n",
    "\n",
    "Memory requirements, at least via spacy-llm, seem really high for running local models\n",
    "\n",
    "unclear if that's because of inefficiencies stemming from spacy library, or inherent to running models\n",
    "\n",
    "wrapping models in libraries seems to generate inefficient use of api's, getting lots of rate limit errors etc with spacy and outline\n",
    "\n",
    "lm studio seems like a good approach to doing openai-like calls without incurring cost or rate limits\n",
    "\n",
    "### renting gpus\n",
    "\n",
    "paperspace is probably still best vs other options, best ui, relatively easy to get notebooks running. some thrash in disconnected kernels seeming to continue to run workloads\n",
    "\n",
    "azure is very enterprisey still, not friendly to solo dev\n",
    "\n",
    "google collab is underbaked, keeps you in their sub-par notebook environment. feels like abandonware/promotionware\n",
    "\n",
    "probably the pricing model everyone lands on is subscription. access to higher GPU RAM machines is gated on higher subscription costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from time import sleep\n",
    "from openai import OpenAI\n",
    "\n",
    "CONFIG_CONTENT = \"\"\"\n",
    "\n",
    "[nlp]\n",
    "lang = \"en\"\n",
    "pipeline = [\"ner\"]\n",
    "\n",
    "[components]\n",
    "\n",
    "[components.ner]\n",
    "source = \"en_core_web_md\"\n",
    "\n",
    "\n",
    "[initialize]\n",
    "vectors = \"en_core_web_md\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open(\"config.cfg\", \"w\") as f:\n",
    "    f.write(CONFIG_CONTENT)\n",
    "\n",
    "\n",
    "DATA_SOURCE_DIR = \"\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"notebook.log\",\n",
    "    filemode=\"a\",\n",
    "    format=\"%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    level=logging.DEBUG,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-11 17:50:38,829] [DEBUG] No 'get_examples' callback provided to 'Language.initialize', creating dummy examples\n",
      "[2024-05-11 17:50:38,834] [INFO] Created vocabulary\n",
      "[2024-05-11 17:50:40,518] [INFO] Added vectors: en_core_web_md\n",
      "[2024-05-11 17:50:40,619] [INFO] Finished initializing nlp object\n",
      "[2024-05-11 17:50:42,172] [DEBUG] No 'get_examples' callback provided to 'Language.initialize', creating dummy examples\n",
      "[2024-05-11 17:50:42,176] [INFO] Created vocabulary\n",
      "[2024-05-11 17:50:44,071] [INFO] Added vectors: en_core_web_md\n",
      "[2024-05-11 17:50:44,148] [INFO] Finished initializing nlp object\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import spacy_llm\n",
    "from spacy_llm.util import assemble\n",
    "import spacy\n",
    "\n",
    "\n",
    "config = \"config.cfg\"\n",
    "\n",
    "model_name = \"en_core_web_md\"\n",
    "try:\n",
    "    nlp = assemble(config)\n",
    "except OSError:\n",
    "    spacy.cli.download(model_name)\n",
    "    print(\"restart and rerun!\")\n",
    "    exit()\n",
    "\n",
    "# set log level to stream to STDOUT\n",
    "spacy_llm.logger.addHandler(logging.StreamHandler())\n",
    "spacy_llm.logger.setLevel(logging.DEBUG)\n",
    "\n",
    "nlp = assemble(\"config.cfg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial pass, identify labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:01<00:00, 123.66it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_PATH = \"msgs.pkl\"\n",
    "\n",
    "with open(DATA_PATH, \"rb\") as f:\n",
    "    docs_df = pd.read_pickle(f)\n",
    "\n",
    "docs = docs_df[0].tolist()\n",
    "\n",
    "\n",
    "entities_rows = []\n",
    "for doc in tqdm(docs):\n",
    "    enriched_doc = nlp(doc)\n",
    "    ents = enriched_doc.ents\n",
    "    for ent in ents:\n",
    "        entities_rows.append({\"name\": ent.text, \"label\": ent.label_, \"fact\": doc, \"enriched_doc\": enriched_doc})  # type: ignore\n",
    "\n",
    "\n",
    "raw_entities_df = pd.DataFrame(entities_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine labels\n",
    "\n",
    "redo labels along customized labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "693it [05:14,  2.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# refine entity labels\n",
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "# os.environ[\"OPENAI_BASE_URL\"] = \"http://localhost:1234/v1\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"lm-studio\"\n",
    "client = OpenAI()\n",
    "# model = client.models.list().data[0].id\n",
    "MODEL = \"gpt-3.5-turbo-instruct\"\n",
    "\n",
    "VALID_LABELS = [\n",
    "    \"PERSON\",\n",
    "    \"PET\",\n",
    "    \"ORG\",\n",
    "    \"PRODUCT\",\n",
    "    \"WEBSITE\",\n",
    "    \"GPE\",\n",
    "    \"TVSHOW\",\n",
    "    \"BOOK\",\n",
    "    \"MOVIE\",\n",
    "    \"TECHNICAL_CONCEPT\",\n",
    "    \"MUSICAL_GROUP\",\n",
    "    \"EVENT\",\n",
    "]\n",
    "\n",
    "TRUST_LABELS = [\"CARDINAL\", \"DATE\", \"TIME\"]\n",
    "\n",
    "entity_rows = []\n",
    "for _, row in tqdm(raw_entities_df.iterrows()):\n",
    "    name = row[\"name\"]\n",
    "    fact = row[\"fact\"]\n",
    "    prev_label = row[\"label\"]\n",
    "\n",
    "    if name == \"Tom\":\n",
    "        entity_rows.append({\"name\": name, \"label\": \"PERSON\", \"fact\": fact, \"score\": 100, \"llm_response\": \"skipping\"})\n",
    "        continue\n",
    "    elif name == \"Sam\":\n",
    "        entity_rows.append({\"name\": name, \"label\": \"AI_ASSISTANT\", \"fact\": fact, \"score\": 100, \"llm_response\": \"skipping\"})\n",
    "    elif prev_label in TRUST_LABELS:\n",
    "        entity_rows.append({\"name\": name, \"label\": prev_label, \"fact\": fact, \"score\": 100, \"llm_response\": \"trusting prev label\"})\n",
    "        continue\n",
    "\n",
    "    label_choices = VALID_LABELS.copy()\n",
    "    random.shuffle(label_choices)\n",
    "    labels_str = \", \".join(label_choices)\n",
    "    candidates = {}\n",
    "\n",
    "    prompt = f\"\"\"You are an entity resolution assistant. \n",
    "    You must classify the entity with name = {name}\n",
    "    \n",
    "    The valid choices are: {labels_str}\n",
    "    \n",
    "    \n",
    "    Use both your inherent knowledge, and this facts derived from chat logs:\n",
    "    {fact}\n",
    "    \n",
    "    Your response should begin with just one word from the following choices: {labels_str}.\n",
    "    Then, a score from 1 to 100, where 100 is the most confident and 1.\n",
    "    Then should follow with a short explanation of your reasoning.\n",
    "    \"\"\"\n",
    "\n",
    "    response = get_openai_response(prompt)\n",
    "    logging.info(f\"***\\nname: {name}\\n\\nfact: {fact}\\n\\nresponse: {response}\\n\\n***\\n\")\n",
    "\n",
    "    # whichever label appears first wins\n",
    "    winner = \"None\"\n",
    "    min_idx = float(\"inf\")\n",
    "    for label in label_choices:\n",
    "        if label in response:\n",
    "            idx = response.index(label)\n",
    "            if idx < min_idx:\n",
    "                min_idx = idx\n",
    "                winner = label\n",
    "\n",
    "    try:\n",
    "        score = int(re.search(r\"\\d+\", response).group())\n",
    "    except AttributeError:\n",
    "        score = 0\n",
    "\n",
    "    entity_rows.append({\"name\": name, \"label\": winner, \"fact\": fact, \"score\": score, \"llm_response\": response})\n",
    "\n",
    "with open(os.path.join(PKL_DIR, \"refined_entity_labels.pkl\"), \"wb\") as f:\n",
    "    pd.DataFrame(entity_rows).to_pickle(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# determine winning labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(os.path.join(PKL_DIR, \"refined_entity_labels.pkl\"), \"rb\") as f:\n",
    "    refined_entity_labels = pd.read_pickle(f)\n",
    "\n",
    "# normalize scores\n",
    "min_score = refined_entity_labels[\"score\"].min() - 1\n",
    "max_score = refined_entity_labels[\"score\"].max()\n",
    "refined_entity_labels[\"score\"] = (refined_entity_labels[\"score\"] - min_score) / (max_score - min_score)\n",
    "\n",
    "\n",
    "entity_names_and_labels_summed_scores = refined_entity_labels.groupby([\"name\", \"label\"]).agg({\"score\": \"sum\"}).reset_index()\n",
    "entity_name_total_scores = refined_entity_labels.groupby(\"name\")[\"score\"].sum().reset_index(\"name\")\n",
    "entity_name_counts = refined_entity_labels.groupby(\"name\")[\"label\"].count().reset_index(\"name\")\n",
    "\n",
    "\n",
    "merged_df = pd.merge(entity_names_and_labels_summed_scores, entity_name_total_scores, on=\"name\", suffixes=(\"\", \"_total_by_name\"))\n",
    "merged_df = pd.merge(merged_df, entity_name_counts, on=\"name\", suffixes=(\"\", \"_count_by_name\"))\n",
    "merged_df[\"confidence_score\"] = merged_df[\"score\"] / merged_df[\"score_total_by_name\"]\n",
    "\n",
    "# remove non-winners\n",
    "final_entity_with_labels = merged_df.sort_values(\"confidence_score\", ascending=False).drop_duplicates(\"name\").reset_index()\n",
    "\n",
    "with open(os.path.join(PKL_DIR, \"final_entity_with_labels.pkl\"), \"wb\") as f:\n",
    "    final_entity_with_labels.to_pickle(f)\n",
    "\n",
    "# # facts and set of entities\n",
    "facts_with_entities = refined_entity_labels.groupby(\"fact\")[\"name\"].apply(lambda x: set(x)).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine raw relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "141it [01:21,  1.74it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "ENTITY_LABEL_CONFIDENCE_THRESHOLD = 0.7\n",
    "\n",
    "relation_rows = []\n",
    "for _, row in tqdm(facts_with_entities.iterrows()):\n",
    "    fact = row[\"fact\"]\n",
    "    entities = row[\"name\"]\n",
    "\n",
    "    if len(entities) < 2:\n",
    "        continue\n",
    "\n",
    "    for entity_1 in entities:\n",
    "        for entity_2 in entities:\n",
    "            if entity_1 == entity_2:\n",
    "                continue\n",
    "            label_1 = final_entity_with_labels[final_entity_with_labels[\"name\"] == entity_1][\"label\"].values[0]\n",
    "            label_2 = final_entity_with_labels[final_entity_with_labels[\"name\"] == entity_2][\"label\"].values[0]\n",
    "\n",
    "            # confidence scores:\n",
    "            score_1 = final_entity_with_labels[final_entity_with_labels[\"name\"] == entity_1][\"confidence_score\"].values[0]\n",
    "            score_2 = final_entity_with_labels[final_entity_with_labels[\"name\"] == entity_2][\"confidence_score\"].values[0]\n",
    "\n",
    "            if score_1 < ENTITY_LABEL_CONFIDENCE_THRESHOLD or score_2 < ENTITY_LABEL_CONFIDENCE_THRESHOLD:\n",
    "                logging.info(f\"skipping {entity_1} and {entity_2} due to low confidence scores\")\n",
    "                continue\n",
    "\n",
    "            relationship_choices = VALID_RELATIONSHIPS.get((label_1, label_2), [])\n",
    "            if len(relationship_choices) == 0:\n",
    "                continue\n",
    "\n",
    "            logging.info(f\"evaluation relationship between {entity_1} and {entity_2}\")\n",
    "\n",
    "            random.shuffle(relationship_choices)\n",
    "\n",
    "            prompt = f\"\"\"You are an entity resolution assistant. \n",
    "            You must classify the relationship between two entities:\n",
    "            Entity 1: name = {entity_1}, type = {label_1} \n",
    "            Entity 2: name = {entity_2}, type = {label_2}\n",
    "        \n",
    "            The valid choices are: {relationship_choices}. If none fit specify NONE.\n",
    "            \n",
    "            Use both your inherent knowledge, and this fact derived from chat logs:\n",
    "            {fact}\n",
    "            \n",
    "            Your response begin with one of the following choices: {relationship_choices}, NONE. \n",
    "            A score from 1 to 100 should follow. 100 means you are very confident in your choice, 1 means you are not confident at all.\n",
    "            Then it should follow with a short explanation of your reasoning.\n",
    "            \"\"\"\n",
    "            response = get_openai_response(prompt)\n",
    "\n",
    "            # first choice to appear in response wins\n",
    "            winner = None\n",
    "            min_idx = float(\"inf\")\n",
    "            for label in relationship_choices:\n",
    "                if label in response:\n",
    "                    idx = response.index(label)\n",
    "                    if idx < min_idx:\n",
    "                        min_idx = idx\n",
    "                        winner = label\n",
    "            # score is regex match for first number in the response\n",
    "            try:\n",
    "                score = int(re.search(r\"\\d+\", response).group())  # type: ignore\n",
    "            except AttributeError:\n",
    "                score = 0\n",
    "\n",
    "            row = {\n",
    "                \"entity_1\": entity_1,\n",
    "                \"entity_2\": entity_2,\n",
    "                \"relationship\": winner,\n",
    "                \"fact\": fact,\n",
    "                \"score\": score,\n",
    "                \"llm_response\": response,\n",
    "            }\n",
    "            relation_rows.append(row)\n",
    "\n",
    "relation_df = pd.DataFrame(relation_rows)\n",
    "with open(os.path.join(PKL_DIR, \"raw_relationships.pkl\"), \"wb\") as f:\n",
    "    relation_df.to_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# normalize scores and pick winners\n",
    "with open(os.path.join(PKL_DIR, \"raw_relationships.pkl\"), \"rb\") as f:\n",
    "    raw_relationships = pd.read_pickle(f)\n",
    "\n",
    "min_score = raw_relationships[\"score\"].min() - 1\n",
    "max_score = raw_relationships[\"score\"].max()\n",
    "raw_relationships[\"score\"] = (raw_relationships[\"score\"] - min_score) / (max_score - min_score)\n",
    "\n",
    "# calculate winning classification and confidence score\n",
    "relationship_names_and_labels_summed_scores = (\n",
    "    raw_relationships.groupby([\"entity_1\", \"entity_2\", \"relationship\"]).agg({\"score\": \"sum\"}).reset_index()\n",
    ")\n",
    "relationship_name_total_scores = raw_relationships.groupby([\"entity_1\", \"entity_2\"])[\"score\"].sum().reset_index()\n",
    "\n",
    "relationship_names_and_labels_summed_scores[\"total_scores_for_entity_pair\"] = relationship_names_and_labels_summed_scores.groupby(\n",
    "    [\"entity_1\", \"entity_2\"]\n",
    ")[\"score\"].transform(\"sum\")\n",
    "\n",
    "relationship_names_and_labels_summed_scores[\"confidence_score\"] = (\n",
    "    relationship_names_and_labels_summed_scores[\"score\"] / relationship_names_and_labels_summed_scores[\"total_scores_for_entity_pair\"]\n",
    ")\n",
    "\n",
    "# finally, limit to winners only, keep confidence score\n",
    "relationship_with_labels = (\n",
    "    relationship_names_and_labels_summed_scores.sort_values(\"confidence_score\", ascending=False)\n",
    "    .drop_duplicates([\"entity_1\", \"entity_2\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# drop extraneous columns\n",
    "final_relationships = relationship_with_labels.drop(columns=[\"index\", \"score\", \"total_scores_for_entity_pair\"]).reset_index()\n",
    "\n",
    "with open(os.path.join(PKL_DIR, \"final_relationships.pkl\"), \"wb\") as f:\n",
    "    final_relationships.to_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import List"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
