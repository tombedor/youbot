{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import spacy\n",
    "import spacy_llm\n",
    "from spacy_llm.util import assemble\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# add a logger to file notebook.log\n",
    "spacy_llm.logger.addHandler(logging.FileHandler(\"notebook.log\"))\n",
    "spacy_llm.logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "nlp = assemble(\"8.cfg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform NER and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/193 [00:00<?, ?it/s]0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      " 13%|█▎        | 25/193 [03:16<12:06,  4.32s/it]  "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "from youbot.experiments.pickler import pickled_cache\n",
    "from youbot.store import Store\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "@pickled_cache\n",
    "def get_data():\n",
    "    docs = Store().get_archival_messages()\n",
    "    return pd.DataFrame(docs)\n",
    "\n",
    "\n",
    "if os.path.exists('entities.pkl') and os.path.exists('relations.pkl'):\n",
    "    with open('entities.pkl', 'rb') as f:\n",
    "        entities_df = pd.read_pickle(f)\n",
    "    with open('relations.pkl', 'rb') as f:\n",
    "        relations_df = pd.read_pickle(f)\n",
    "    \n",
    "else:\n",
    "    docs_df = get_data()\n",
    "    docs = docs_df[0].tolist()\n",
    "    \n",
    "    \n",
    "    entities_rows = []\n",
    "    relations_rows = []\n",
    "    for doc in tqdm(docs):\n",
    "        enriched_doc = nlp(doc)\n",
    "        sleep(0.1) # sleep for openai\n",
    "        ents = enriched_doc.ents\n",
    "        for ent in ents:\n",
    "            entities_rows.append({\"name\": ent.text, \"label\": ent.label_, \"fact\": doc})  # type: ignore\n",
    "        for rel in enriched_doc._.rel:\n",
    "            dep_name = ents[rel.dep].text\n",
    "            dep_label = ents[rel.dep].label_\n",
    "            dest_name = ents[rel.dest].text\n",
    "            dest_label = ents[rel.dest].label_\n",
    "\n",
    "            # ignore self relations\n",
    "            if (dep_name, dep_label) == (dest_name, dest_label):\n",
    "                continue\n",
    "\n",
    "            # ignore dates\n",
    "            if dep_label == \"DATE\" or dest_label == \"DATE\":\n",
    "                continue\n",
    "\n",
    "            relations_rows.append(\n",
    "                {\n",
    "                    \"dep_name\": dep_name,\n",
    "                    \"dep_label\": dep_label,\n",
    "                    \"dest_name\": dest_name,\n",
    "                    \"dest_label\": dest_label,\n",
    "                    \"rel\": rel.relation,\n",
    "                    \"fact\": doc,\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "    entities_df = pd.DataFrame(entities_rows)\n",
    "    relations_df = pd.DataFrame(relations_rows)\n",
    "\n",
    "    with open('entities.pkl', 'wb') as f:\n",
    "        entities_df.to_pickle(f)\n",
    "        \n",
    "    with open('relations.pkl', 'wb') as f:\n",
    "        relations_df.to_pickle(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolve inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group facts from entities\n",
    "grouped_entities_df = entities_df.groupby([\"name\", \"label\"]).agg({\"fact\": lambda x: list(x)}).reset_index()\n",
    "grouped_relations_df = (\n",
    "    relations_df.groupby([\"dep_name\", \"dep_label\", \"dest_name\", \"dest_label\", \"rel\"]).agg({\"fact\": lambda x: list(x)}).reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "# in some passes, a subset of the labels should be discarded\n",
    "\n",
    "# perhaps, characterize the convo by work, personal, etc and tailor retrieval accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clashing entity types\n",
    "labels_grouped = grouped_entities_df.groupby([\"name\"]).agg({\"label\": lambda x: list(x), \"fact\": lambda x: [item for sublist in x for item in sublist]}).reset_index()\n",
    "clashing_entity_labels = labels_grouped[labels_grouped[\"label\"].apply(lambda x: len(x) > 1)]\n",
    "\n",
    "\n",
    "rels_grouped = grouped_relations_df.groupby([\"dep_name\", \"dest_name\"]).agg({\"rel\": lambda x: list(x), \"fact\": lambda x: [item for sublist in x for item in sublist]}).reset_index()\n",
    "clashing_rels = rels_grouped[rels_grouped[\"rel\"].apply(lambda x: len(x) > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/tombedor/.cache/huggingface/token\n",
      "Login successful\n",
      "Pydantic: Choices = ['NORP', 'ORG']. WINNER = NORP\n",
      "Python: Choices = ['ORG', 'PRODUCT']. WINNER = PRODUCT\n",
      "Rocky: Choices = ['GPE', 'PERSON']. WINNER = GPE\n",
      "SQLAlchemy: Choices = ['PRODUCT', 'WORK_OF_ART']. WINNER = PRODUCT\n",
      "SQLModel: Choices = ['ORG', 'PERSON']. WINNER = ORG\n",
      "asyncio: Choices = ['GPE', 'PERSON']. WINNER = GPE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import huggingface_hub\n",
    "import outlines.models.openai\n",
    "from pandas import Series\n",
    "# resolve entities\n",
    "import outlines\n",
    "import outlines.models\n",
    "from pandas import Series\n",
    "huggingface_hub.login(token=os.environ['HF_TOKEN'])\n",
    "model = outlines.models.openai(\"gpt-3.5-turbo\") \n",
    "# model = outlines.models.openai(\"gpt-4-0613\")\n",
    "# model = outlines.models.transformers(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "# model = outlines.models.llamacpp(\"TheBloke/phi-2-GGUF\", \"phi-2.Q4_K_M.gguf\")\n",
    "\n",
    "\n",
    "def pick_winning_label(row: Series) -> str:\n",
    "    labels = row[\"label\"]\n",
    "    name = row[\"name\"]\n",
    "    facts = \"\\n\".join(row[\"fact\"])\n",
    "\n",
    "    prompt = f\"\"\"You are an entity resolution assistant. \n",
    "    You must classify the entity with name = {name}\n",
    "    \n",
    "    Use both your inherent knowledge, and these facts derived from chat logs:\n",
    "    {facts} \n",
    "    \"\"\"\n",
    "\n",
    "    generator = outlines.generate.choice(model, labels)\n",
    "    answer = generator(prompt)\n",
    "    print(f\"{name}: Choices = {labels}. WINNER = {answer}\")\n",
    "    return answer\n",
    "\n",
    "\n",
    "new_df = clashing_entity_labels.apply(pick_winning_label, axis=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
